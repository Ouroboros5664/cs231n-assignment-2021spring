## Dropout & Inverted Dropout

放两个数学公式就清楚了，既然我们在每一层以$p$的概率保留神经元，那么假设原先这一层的输出是$a$，那么自然现在的输出期望就变成了$pa$

传统的Dropout在test的时候是不做mask处理的，那么此时的输出期望仍然是$a$，但我们需要他跟训练的时候保持一致，因此需要对输出处理一下，变成$pa$

而Inverted Dropout正好相反，我们希望在test的时候什么都不做，那么自然就想把这件事情放在train的时候处理掉，于是对于期望输出为$pa$，除以一个$p$即可